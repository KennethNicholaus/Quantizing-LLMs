{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golongson/miniconda3/envs/eval/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7b2b4454bc30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "torch.manual_seed(1230)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_it(start,end):\n",
    "    nano = end-start\n",
    "    return nano/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "max_token = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Precision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golongson/miniconda3/envs/eval/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/golongson/miniconda3/envs/eval/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 4,423,265,024 bytes\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a software engineer. I have been working in the software industry for the past 5 years and have experience in developing web applications using various technologies such as Java, JavaScript, and HTML. I am proficient in using tools such as Git, JIRA, and Slack to manage projects and communicate with team members. I am also skilled in designing and implementing user-friendly interfaces using CSS and HTML. In my free time, I enjoy playing video games, reading books, and spending time with my family and friends. I am passionate about learning new technologies and staying up-to-date with the latest trends in the industry. I am looking forward to working with you and contributing to the development of the project. Thank you for considering my application. Best regards,\n",
      "\n",
      "[Your Name]\n",
      "Seconds: 7.625865433\n",
      "Token/s 23.735011008290893\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INT 8 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1,242,749,696 bytes\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "   load_in_8bit=True,\n",
    "   bnb_8bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_8bit.get_memory_footprint():,} bytes\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John and I am a software engineer. I have been working in the software industry for the past 10 years. I have worked on various projects, including web development, mobile app development, and data analysis. I have a Bachelor's degree in Computer Science from the University of California, Berkeley. In my free time, I enjoy playing video games, reading books, and spending time with my family and friends. I am passionate about learning new technologies and staying up-to-date with the latest trends in the industry. I am also interested in entrepreneurship and have started my own company, which I am currently working on. I am looking forward to working with you and helping you achieve your goals. Thank you for considering my application. I hope to hear from you soon. Best regards,\n",
      "\n",
      "[Your Name]\n",
      "Seconds: 29.420471193\n",
      "Token/s 6.152178828565643\n",
      "CPU times: user 27.7 s, sys: 1.38 s, total: 29.1 s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_8bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_8bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INT4 Quantization FP4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 758,307,584 bytes\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_4bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John and I am 25 years old. I am a student and I am studying in the University of London. I am a very enthusiastic and motivated person. I am very interested in sports and fitness. I have a passion for fitness and I love to work out. I am a very hardworking person and I am always looking for ways to improve my fitness. I am a very active person and I love to go out and exercise. I am very passionate about fitness and I love to share my knowledge and experience with others. I am a very friendly and outgoing person and I love to meet new people and make new friends. I am a very positive and optimistic person and I always see the best in people. I am a very hardworking person and I always put in the effort to achieve my goals. I am a very determined person and I always have a positive attitude towards life. I am a very creative and artistic person and\n",
      "Seconds: 10.990910984\n",
      "Token/s 18.651775116587554\n",
      "CPU times: user 11.1 s, sys: 281 ms, total: 11.3 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INT4 Quantization NF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 758,307,584 bytes\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_4bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a student at the University of XYZ. I am currently enrolled in the Bachelor of Science in Computer Science program. I am currently taking 12 credits and have completed 10 credits. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project\n",
      "Seconds: 10.868202905\n",
      "Token/s 18.862364071771992\n",
      "CPU times: user 11 s, sys: 287 ms, total: 11.3 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested 4Bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 758,307,584 bytes\n"
     ]
    }
   ],
   "source": [
    "double_quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_4bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a student at the University of XYZ. I am currently enrolled in the Bachelor of Science in Computer Science program. I am currently taking 12 credits and have completed 10 credits. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project\n",
      "Seconds: 11.767528937\n",
      "Token/s 17.42081970628767\n",
      "CPU times: user 10.8 s, sys: 279 ms, total: 11.1 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Quantization feature together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 758,307,584 bytes\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
    "print(f\"Model size: {model_4bit.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John Smith and I am a student at the University of XYZ. I am currently enrolled in the Bachelor of Science in Computer Science program. I am currently taking 12 credits and have completed 10 credits. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project for my final project. I am currently working on a project\n",
      "Seconds: 10.655001702\n",
      "Token/s 19.23979045085656\n",
      "CPU times: user 10.9 s, sys: 235 ms, total: 11.1 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "start = time.time_ns()\n",
    "outputs = model_4bit.generate(**inputs, max_new_tokens=max_token)\n",
    "end = time.time_ns()\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "t = time_it(start,end)\n",
    "print(\"Seconds:\",t)\n",
    "print(\"Token/s\",len(outputs[0])/t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Quant | Memory (GB) | Inference (Tokens/s) |\n",
    "| ------ | -------- | ------- |\n",
    "| Full Precision | 4,4 | 40.91 |\n",
    "| 8bit | 1,2 | 8.2 |\n",
    "| 4 bit FP4 | 0.750 | 20.72 | \n",
    "| 4 bit Normal Float 4 | 0.750 |19.77 |\n",
    "| Nested 4 bit | 0.758 | 20.21 | \n",
    "| All together | 0.750 | 21.98 | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eval] *",
   "language": "python",
   "name": "conda-env-eval-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
